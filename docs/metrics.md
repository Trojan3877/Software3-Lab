# 📊 Software-3.0 Lab — Metrics Dashboard

This page aggregates key performance indicators from each fine-tuned model and evaluation cycle.  
All raw numbers come from **`data/eval/results/*.json`** (generated by `pipelines/evaluate.py`) and are ingested nightly into **Snowflake** for long-term analytics and cost tracking.

---

## 1 – Latest Evaluation Snapshot

| Timestamp (UTC) | Model | Rouge-L | BLEU-4 | Pass@3 | #Samples | Notes |
|-----------------|-------|---------|--------|--------|----------|-------|
| 2025-06-30 14:12 | `ft:gpt-3.5-turbo:2025-06-30` | **0.46** | **0.34** | **0.78** | 120 | Baseline fine-tune v1 |
| _← append rows on each run_ | | | | | | |

> **Pass Criteria**  
> *Target Rouge-L ≥ **0.45***, BLEU-4 ≥ **0.30**, Pass@3 ≥ **0.75**.  
> Models failing thresholds trigger a GitHub Actions alert.

---

## 2 – Cost & Latency Trends (Snowflake View)

| Metric | 7-Day Avg | 30-Day Trend | Source |
|--------|-----------|--------------|--------|
| **Fine-Tune Cost / run** | $**1.87** | ↘︎ 12 % | OpenAI Usage API |
| **Inference Latency (P95)** | **2.8 s** | ↗︎ 8 % | FastAPI logs |
| **Snowflake Credits / eval** | **0.0014** | ↘︎ 5 % | `WAREHOUSE_METERING_HISTORY` |

---

## 3 – Grafana Dashboards

| Dashboard | Link |
|-----------|------|
| **Model Metrics** | <https://grafana.example.com/d/software3/model-metrics> |
| **Cost Explorer** | <https://grafana.example.com/d/software3/cost> |
| **OpenTelemetry Traces** (planned) | _TBD_ |

---

## 4 – How Data Flows

1. `pipelines/evaluate.py` writes JSON metrics → `data/eval/results`.
2. GitHub Actions nightly cron job (`.github/workflows/metrics-export.yml`) pushes aggregated rows to **Snowflake** (`SOFTWARE3_METRICS.PUBLIC.EVAL_RUNS`).
3. Grafana dashboards read from Snowflake via Postgres connector.
4. README badge refreshes with Rouge-L and cost deltas (coming soon).

---

_Last updated: {{DATE}}_
